import numpy as np
from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, BatchNormalization
from tensorflow.keras.layers import LeakyReLU
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
from data_preprocessing import load_and_preprocess_data

def build_generator():
    """
    Builds the Generator model for the GAN.
    """
    model = Sequential()
    model.add(Dense(256, input_dim=100))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(1024))
    model.add(LeakyReLU(alpha=0.2))
    model.add(BatchNormalization(momentum=0.8))
    model.add(Dense(1000, activation='tanh'))
    model.add(Reshape((1000, 1)))
    return model

def build_discriminator():
    """
    Builds the Discriminator model for the GAN.
    """
    model = Sequential()
    model.add(Flatten(input_shape=(1000, 1)))
    model.add(Dense(512))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(256))
    model.add(LeakyReLU(alpha=0.2))
    model.add(Dense(1, activation='sigmoid'))
    return model

def train_gan(epochs=10000, batch_size=32):
    """
    Trains the GAN to generate synthetic ABP data.
    """
    # Load and preprocess the data
    _, abp_data = load_and_preprocess_data()
    X_train = abp_data[:9600]

    # Build and compile the models
    optimizer = Adam(0.0002, 0.5)

    discriminator = build_discriminator()
    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    generator = build_generator()

    z = Input(shape=(100,))
    generated_signal = generator(z)

    discriminator.trainable = False
    validity = discriminator(generated_signal)

    combined = Model(z, validity)
    combined.compile(loss='binary_crossentropy', optimizer=optimizer)

    # Training loop
    for epoch in range(epochs):
        # Train Discriminator
        idx = np.random.randint(0, X_train.shape[0], batch_size)
        real_signals = X_train[idx]

        noise = np.random.normal(0, 1, (batch_size, 100))
        generated_signals = generator.predict(noise)

        d_loss_real = discriminator.train_on_batch(real_signals, np.ones((batch_size, 1)))
        d_loss_fake = discriminator.train_on_batch(generated_signals, np.zeros((batch_size, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train Generator
        noise = np.random.normal(0, 1, (batch_size, 100))
        g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))

        # Print progress
        print(f"{epoch} [D loss: {d_loss[0]:.4f}, acc.: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")

if __name__ == '__main__':
    train_gan()
